---
title: 03.Bayesian networks(W4)
index: true
icon: "/assets/icon/common/slides.svg"
icon-size: "4rem"
author: Haiyue
date: 2023-08-25
category:
  - Guide
---

## Bayesian networkds learning

### Search and Score based methods
* Search for all possible DAGs
* Score each DAG with a scoring function
* The DAG with the highest score (best fit the data)
* NP-hard problem
::: tabs
@tab How many DAGs Must be scored?
$f(n) = \displaystyle\sum_{i=1}^n(-1)^{i+1}\dbinom{n}{i}2^{i(n-i)}f(n-i), n > 2$

[View the code here](https://colab.research.google.com/drive/1SmIaE1FDEtGrTzbuoEzOuOhS1Z59j2sn?usp=sharing)
``` python
import numpy as np
from functools import *
import math

def dag_count(n):
  ret = 0
  if 0 == n or 1 == n:
    return 1
  if 2 == n:
    return 3
  if n >= 2:
    for i in range(1, n+1):
      ret += ((-1)**(i+1)) * math.comb(n, i) * (2**(i*(n-i))) * dag_count(n-i)
  return ret
```
$f(2) = 3$
$f(3) = 25$
$f(5) = 29000$
$f(10) = 4.2 * 10^{18}$

@tab How to search
Suppose we want to determine whether job status $(J)$ has a <span style="color:orange">causal effect</span> on whether someone <span style="color:orange">defaults on a loan $(F)$</span>. Furthermore, we articulate just two values for each variable as follows:
![Search and Score](/data/unisa/AdvancedAnalytic2/week4/searchandscore.png =400x240)

@tab How to Score

The <span style="color:orange;font-weight:fold;">Bayesian information criterion (BIC)</span> score is as follow:
$BIC(G:D) = ln(P(D|\hat{P}, G)) - \frac{d}{2}ln(m)$

<span style="color:orange;font-weight:fold;">$m$:</span> the number of data items,
<span style="color:orange;font-weight:fold;">$d$:</span> the dimension of the DAG model
<span style="color:orange;font-weight:fold;">$\hat{P}$:</span> the set of maximum likelihood values of the parameters. 
The dimension is the number of parameters in the model.

The BIC score is intuitively appealing because it contains 
<span style="color:orange">(1)</span>: a term that shows how well the model predicts the data when the parameter set is equal to its ML value, and 
<span style="color:orange">(2)</span>: a term that punishes for model complexity.
Another nice feature of the BIC is that it does not depend on the prior distribution of the parameters, which means there is no need to assess one.
:::

#### Examples
::: tabs

@tab Graph 1
![Graph 1](/data/unisa/AdvancedAnalytic2/week4/scoreexample.png =400x200) {align="left"}

$$
\begin{equation}
\begin{split}
\hat{P}(j_1)     &= \frac{5}{8}\\
\hat{P}(f_1|j_1) &= \frac{4}{5}\\
\hat{P}(f_1|j_2) &= \frac{1}{3}\\
  &\Downarrow \\
  \textcolor{red}{P(D|\hat{P},G_1)} &= [\hat{P}(f_1|j_1)\hat{P}(j_1)]^4[\hat{P}(f_2|j_1)\hat{P}(j_1)][\hat{P}(f_1|j_2)\hat{P}(j_2)][\hat{P}(f_2|j_2)\hat{P}(j_2)]^2\\
    & = (\frac{4}{5}*\frac{5}{8})^4(\frac{1}{5}*\frac{5}{8})(\frac{1}{3}*\frac{3}{8})(\frac{2}{3}*\frac{3}{8})^2\\
      &=6.1035*10^{-5}\\
  &\Downarrow \\
  \textcolor{red}{BIC(G_1:D)} &= ln(P(D|\hat{P}, G_1)) \frac{d}{2}ln(m)\\
   &= ln(6.1035*10^{-5})-\frac{3}{2}ln(8)\\
   &= -12.823
\end{split}
\end{equation}
$$


@tab Graph 2
![Graph 2](/data/unisa/AdvancedAnalytic2/week4/scoreexample2.png)
$$
\begin{equation}
\begin{split}
  \hat{P}(j_1) &= \frac{5}{8}\\
  \hat{P}(f_1) &= \frac{5}{8}\\
  &\Downarrow \\
  \textcolor{red}{P(D|\hat{P}, G_2)} &= [\hat{P}(f_1)\hat{P}(j_1)]^4[\hat{P}(f_2)\hat{P}(j_1)][\hat{P}(f_1)\hat{P}(j_2)][\hat{P}(f_2)\hat{P}(j_2)]^2\\
  &= (\frac{5}{8}*\frac{5}{8})^4(\frac{3}{8}*\frac{5}{8})(\frac{5}{8}*\frac{3}{8})(\frac{3}{8}*\frac{3}{8})^2\\
  &= 2.5292*10^{-5}\\
  &\Downarrow \\
  \textcolor{red}{BIC(G_2:D)} &= ln(P(D|\hat{P}, G_2)) \frac{d}{2}ln(m)\\
   &= ln(2.5292*10^{-5})-\frac{2}{2}ln(8)\\
   &= -12.644
\end{split}
\end{equation}
$$

:::


### Constraint based approach
* Use statistical tests to evaluate the dependency between variables
* Exponential to the number of nodes

::: tabs
@tab Step1: Correlation
Correlation Graph: 
![Correlation Graph](/data/unisa/AdvancedAnalytic2/week4/CorrelationGraph.png =400x)
* Identify correlations between every pair of variables in the dataset. 
* An edge between 2 nodes represents the correlated pair

@tab Step2: Conditional independence tests
![Correlation Graph 2](/data/unisa/AdvancedAnalytic2/week4/CorrelationGraph2.png =400x)
Test whether <span style="color:orange">$B$</span> and <span style="color:orange">$C$</span> are correlated just because of <span style="color:orange">common cause $A$</span>, we use <span style="color:orange">conditional independence test:  $I(B, C|A)$</span>.
e.g. Partial correlation, Chi-square
![Correlation Graph 3](/data/unisa/AdvancedAnalytic2/week4/CorrelationGraph3.png =400x)

:::

#### PC(Peter & Clark)  algorithm

::: tabs

@tab Step1: Learning the skeleton
![](/data/unisa/AdvancedAnalytic2/week4/pc_step1.png =400x340)
steps:
![](/data/unisa/AdvancedAnalytic2/week4/StepsForStep1.png =400x300)


@tab Step2: Orientating the Edges

![](/data/unisa/AdvancedAnalytic2/week4/pc_step2.png =400x300)
![](/data/unisa/AdvancedAnalytic2/week4/pc_step2-2.png =400x50)

:::


## References
01. **Week 4 Slides from [Thuc](https://people.unisa.edu.au/thuc.le) (SP52023)**
02. [PC 算法 - 贝叶斯网络与其结构学习算法](https://zhuanlan.zhihu.com/p/368010458)