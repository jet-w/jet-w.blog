---
title: 01.Naïve Bayes Classifier
index: true
icon: "/assets/icon/common/data-mining.svg"
icon-size: "4rem"
author: Haiyue
date: 2023-07-31
category:
  - classfier
tag:
  - AI
  - Naïve Bayes
  - Naive Bayes
---

## Background
There are three methods to establish a classifier

A. Model a <span style="color:red">classification rule</span> directly
B. Model the <span style="color:red">probability of class memberships</span> given input data
C. Make a <span style="color:red">probabilistic</span> model of data within each class

Examples
| Method | discriminative classification | generative classification | probabilistic classification | Examples | 
| :--: | :--: | :--: | :--: | -- |
| A | <span style="color:red">Yes</span> | No  | No | k-NN, decision trees, perceptron, SVM|
| B | <span style="color:red">Yes</span> | No  |<span style="color:red">Yes</span>  | perceptron with the cross-entropy cost |
| C | No | <span style="color:red">Yes</span>  |<span style="color:red">Yes</span>  |naive Bayes, model based classifiers|

## Probability Basics
Prior, conditional and joint probability for random variables
- <span style="color:orange;font-weight:bold;">Prior probability:</span> $P(X)$
- <span style="color:orange;font-weight:bold;">Conditional probability:</span> $P(X_1| X_2), P(X_2| X_1)$
- <span style="color:orange;font-weight:bold;">Joint probability:</span> $\chi=(X_1,X_2), P(\chi)=P(X_1,X_2)$
- <span style="color:orange;font-weight:bold;">Relationship:</span> $P(X_1, X_2)=P(X_2|X_1)P(X_1)=P(X_1|X_2)P(X_2)$
- <span style="color:orange;font-weight:bold;">Independence:</span> $P(X_2|X_1)=P(X_2)P(X_1|X_2)=P(X_1)P(X_1,X_2)=P(X_1)P(X_2)$

### Bayes Rule
1. $P(C|\chi)=\frac{P(\chi|C) P(C)}{P(\chi)}$
2. $Posterior=\frac{Likelihood \times  Prior}{Evidence}$

### Quiz
We have two six-sided dice. When they are tolled, it could end up with the following occurance:
A. dice 1 lands on side "3"
B. dice 2 lands on side "1"
C. Two dice sum to eight

Answer the following Questions:
1. $P(A)$ = $\frac{1}{6}$  Independent events.
2. $P(B)$ = $\frac{1}{6}$  Independent events.
3. $P(C)$ = $\frac{1}{6}*\frac{1}{6}*5$  The prior dice will influence another one
4. $P(A|B)$ = $\frac{1}{6}$  Independent events.
5. $P(C|A)$ = $0$  Independent events, (it means the probability of dice 7 lands on side 1).
6. $P(A,B)$ = $P(A)P(B) = \frac{1}{6}*\frac{1}{6}$
7. $P(A,C)$ = $P(A)P(C) = \frac{1}{6}*\frac{1}{6}*5 *\frac{1}{6}$
8. Is $P(A,C)$ equal to $P(A)*P(C)$ ?

## Probabilistic Classification
Establishing a probabilistic model for classification
- Discriminative model
![Discriminative Model](/data/unisa/AdvancedAnalytic2/DiscriminativeModel.jpg)

- Generative model
![Generative Model](/data/unisa/AdvancedAnalytic2/generativemodel.jpg)

### MAP classification rule
- **MAP**: Maximum A Posterior
- **Assign** $x$ to $c^*$ if $P(C=c^*|\Chi=\chi) > P(C=c|\Chi=\chi) c \neq c^*, c=c_1, ..., c_{_L}$

### Generative classification with the MAP rule
- Apply [Bayes rule](#bayes-rule) to convert them into posterior probabilities
$P(C=c_i|\Chi=\chi)=\frac{P(\Chi=\chi|C=c_i)P(C=c_i)}{P(\Chi=\chi)} \propto P(\Chi=\chi| C=c_i)P(C=c_i)$ for i=1,2,..., L

## Naïve Bayes

- **Bayes classification**
$P(C|\Chi) \propto P(\Chi|C)P(C)=P(X_1,\cdot\cdot\cdot, X_n | C) P(C)$
<span style="color:red;font-weight:bold;">Difficulty:</span> learning the joint probability $P(X_1,\cdot\cdot\cdot ,X_n|C)$
- **Naïve Bayes classification**
  Assumption that <span style="color:red">all input features are conditionally independent!</span>
$$\begin{equation}
\begin{split}   
  P(X_1,X_2,\cdot\cdot\cdot,X_n|C) &=P(X_1|X_2,\cdot\cdot\cdot,X_n,C)P(X_2,\cdot\cdot\cdot,X_n|C)\\
    &=P(X_1|C)P(X_2,\cdot\cdot\cdot,X_n|C)\\
    &=P(X_1|C)P(X_2|C)\cdot\cdot\cdot P(X_n|C)
\end{split}
\end{equation}$$
  <span style="color:red;font-weight:bold;">MAP classification rule</span>: for $x=(x_1,x_2,\cdot\cdot\cdot,x_n)$
$[P(x_1|C^*)\cdot\cdot\cdot P(x_n|c^*)]P(c^*)>[P(x_1|c)\cdot\cdot\cdot P(x_n|c)]P(c),c\neq c^*,c=c_1,\cdot\cdot\cdot,c_L$

---

- **Algorithm**: Discrete-Valued Features
  1. **<span style="color:red">Learning Phase</span>**: Given a training set S, 
    For each target value of $c_i(c_i=c_1,\cdot\cdot\cdot,c_{_L})$
    $\hat{P}(C=c_i)\gets$ estimate $P(C=c_i)$ with examples in S;
    for every feature value $\chi_{jk}$ of each feature $X_j(j=1,\cdot\cdot\cdot,n;k=1,\cdot\cdot\cdot,N_j)$
    $\hat{P}(X_j=x_{jk}|C=c_j)\gets$ estimate $P(X_j=x_{jk}|C=c_i)$ with example in $S$;
    **Output**: conditional probability tables; for $X_j,N_j\times L$ elements
  2. **<span style="color:red">Test Phase</span>**: Given an unknown instance $\Chi^{'}=(a_{1}^{'},\cdot\cdot\cdot,a_{n}^{'})$
    Look up tables to assign the label $c^*$ to $\Chi^{'}$ if 
    $[\hat{P}(a_{1}^{'}|c^*)\cdot\cdot\cdot \hat{P}(a_{n}^{'}|c^*)]\hat{P}(c^*) > [\hat{P}(a_{1}^{'}|c)\cdot\cdot\cdot\hat{P}(a_{n}^{'}|c)]\hat{P}(c), c\neq c^*, c=c_1,\cdot\cdot\cdot, c_{_L}$

### Examples: Discrete-Valued Features
::: details Original Data
| Day | Outlook | Temperature | Humidity | Wind | PlayTennis |
| --- | ---      | --- | --- | --- | --- |
| D1  | Sunny    | Hot  | High   | Weak   | No  |
| D2  | Sunny    | Hot  | High   | Strong | No  |
| D3  | <span style="font-weight:bold;color:red">Overcast</span> | <span style="font-weight:bold;color:red">Hot </span> | <span style="font-weight:bold;color:red">High  </span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D4  | <span style="font-weight:bold;color:red">Rain    </span> | <span style="font-weight:bold;color:red">Mild</span> | <span style="font-weight:bold;color:red">High  </span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D5  | <span style="font-weight:bold;color:red">Rain    </span> | <span style="font-weight:bold;color:red">Cool</span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D6  | Rain     | Cool | Normal | Strong | No  |
| D7  | <span style="font-weight:bold;color:red">Overcast</span> | <span style="font-weight:bold;color:red">Cool</span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Strong</span> | <span style="font-weight:bold;color:red">Yes</span> |
| D8  | Sunny    | Mild | High   | Weak   | No  |
| D9  | <span style="font-weight:bold;color:red">Sunny   </span> | <span style="font-weight:bold;color:red">Cool</span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D10 | <span style="font-weight:bold;color:red">Rain    </span> | <span style="font-weight:bold;color:red">Mild</span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D11 | <span style="font-weight:bold;color:red">Sunny   </span> | <span style="font-weight:bold;color:red">Mild</span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Strong</span> | <span style="font-weight:bold;color:red">Yes</span> |
| D12 | <span style="font-weight:bold;color:red">Overcast</span> | <span style="font-weight:bold;color:red">Mild</span> | <span style="font-weight:bold;color:red">High  </span> | <span style="font-weight:bold;color:red">Strong</span> | <span style="font-weight:bold;color:red">Yes</span> |
| D13 | <span style="font-weight:bold;color:red">Overcast</span> | <span style="font-weight:bold;color:red">Hot </span> | <span style="font-weight:bold;color:red">Normal</span> | <span style="font-weight:bold;color:red">Weak  </span> | <span style="font-weight:bold;color:red">Yes</span> |
| D14 | Rain     | Mild | High   | Strong | No  |
:::
::: details Learning Phase
$P(Play = Yes) = \frac{9}{14}$   $P(Play = No) = \frac{5}{14}$
| <span style="font-weight:bold;color:red">Outlook</span> | Play = Yes | Play = No |
| -- | :--: | :--: |
|Sunny | $\frac{2}{9}$ | $\frac{3}{5}$ |
|Overcast | $\frac{4}{9}$ | $\frac{0}{5}$ |
|Rain | $\frac{3}{9}$ | $\frac{2}{5}$ |

| <span style="font-weight:bold;color:red">Temperature</span> | Play = Yes | Play = No |
| -- | :--: | :--: |
|Hot  | $\frac{2}{9}$ | $\frac{2}{5}$ |
|Mild | $\frac{4}{9}$ | $\frac{2}{5}$ |
|Cool | $\frac{3}{9}$ | $\frac{1}{5}$ |

| <span style="font-weight:bold;color:red">Outlook</span> | Play = Yes | Play = No |
| -- | :--: | :--: |
|High   | $\frac{3}{9}$ | $\frac{4}{5}$ |
|Normal | $\frac{6}{9}$ | $\frac{1}{5}$ |

| <span style="font-weight:bold;color:red">Outlook</span> | Play = Yes | Play = No |
| -- | :--: | :--: |
|Strong | $\frac{3}{9}$ | $\frac{3}{5}$ |
|Weak   | $\frac{6}{9}$ | $\frac{2}{5}$ |
:::

::: details Test Phase
- Given a new instance, predict its label
$x' = (Outlook = Sunny, Temperature=Cool, Humidity = High, Wind = Strong)$

- Look up tables achieved in the learning phrase
$P(Outlook = Sunny | Play=Yes) = \frac{2}{9}$   
$P(Outlook=Sunny|Play=No) = \frac{3}{5}$
$P(Temperature=Cool|Play=Yes) = \frac{3}{9}$    
$P(Temperature=Cool|Play==No) = \frac{1}{5}$
$P(Huminity=High|Play=Yes) = \frac{3}{9}$       
$P(Huminity=High|Play=No) = \frac{4}{5}$
$P(Wind=Strong|Play=Yes) = \frac{3}{9}$         
$P(Wind=Strong|Play=No) = \frac{3}{5}$
$P(Play=Yes) = \frac{9}{14}$                    
$P(Play=No) = \frac{5}{14}$

- Decision making with the [MAP](#map-classification-rule) rule
$P(Yes|x'): [P(Sunny|Yes)P(Cool|Yes)P(High|Yes)P(Strong|Yes)]P(Play=Yes) = 0.0053$
$P(No|x'): [P(Sunny|No) P(Cool|No)P(High|No)P(Strong|No)]P(Play=No) = 0.0206$

::: info Final Result
The fact $P(Yes|x') < P(No|x')$, we label $x'$ to be “$No$”.    
:::


- **Algorithm**: Continuous-valued Features
  - Numberless values for a feature
  - Conditional probability often modeled with the normal
  $\hat{P}(X_j|C=c_i)=\frac{1}{\sqrt{2\pi}\sigma_{ji}}exp(-\frac{(X_j-\mu_{ji})^2}{2\sigma_{ji}^{2}})$
      $\mu_{ji}$: mean (average) of feature values $X_j$ of examples for which $C=c_j$
      $\sigma_{ji}$: standard deviation of feature values $\Chi_{j}$ of examples for which $C=c_i$

  1. **<span style="color:red">Learning Phase</span>**: 
    for $\Chi=(X_1,\cdot\cdot\cdot,X_n),C=c_1,\cdot\cdot\cdot,c_{_L}$
    **Output**: $n \times L$ normal distributions and $P(C=c_i) i=1,\cdot\cdot\cdot,L$
  2. **<span style="color:red">Test Phase</span>**: Given an unknown instance $\Chi^{'}=(a_{1}^{'},\cdot\cdot\cdot,a_{n}^{'})$
      - Instead of looking-up tables, calculate conditional probabilities with all the normal distributions achieved in the learning phrase
      - Apply the MAP rule to make a decision

### Example: Continuous-valued Features 
::: details Data
- Temperature is naturally of continuous value.
Yes: 25.2, 19.3, 18.5, 21.7, 20.1, 24.3, 22.8, 23.1, 19.8
No: 27.3, 30.1, 17.4, 29.5, 15.1

- Estimate mean and variance for each class
$\mu = \frac{1}{N}\displaystyle\sum_{n=1}^{N}x_n$
$\sigma^2 = \frac{1}{N}\displaystyle\sum_{n=1}^{N}(x_n-\sigma)^2$

According to the formula above, the result could be get 
$\mu_{yes} = 21.64, \sigma_{yes} = 2.35$
$\mu_{No} = 23.88, \sigma_{No} = 7.09$
:::
::: details Learning Phase
output two Gaussian models for P(temp|C)
$\hat{P}(x | Yes) = \large{\frac{1}{2.35\sqrt{2\pi}}exp\left(-\frac{(x-21.64)^2}{2*2.35^2}\right)} =  \large{\frac{1}{2.35\sqrt{2\pi}}exp\left(-\frac{(x-21.64)^2}{11.09}\right)}$

$\hat{P}(x | No) = \large{\frac{1}{7.09\sqrt{2\pi}}exp\left(-\frac{(x-23.88)^2}{2*7.09^2}\right)} =  \large{\frac{1}{7.09\sqrt{2\pi}}exp\left(-\frac{(x-23.88)^2}{50.25}\right)}$
:::


## Relevant Issues
- Violation of Independence Assumption
    - For many real world tasks, $P(X_1,\cdot\cdot\cdot,X_n|C) \neq P(X_1|C)\cdot\cdot\cdot P(X_n|C)$
    - Nevertheless, naïve Bayes works surprisingly well anyway!
- Zero conditional probability Problem
    - If no example contains the attribute value
    - In this circumstance, $$ during test 
    - For a remedy, conditional probabilities estimated with
        > $\large{\hat{P}(X_j = a_{jk} | C=c_i) = \frac{n_c + mp}{n + m}}$
        > $n_c:$ number of training examples for which $X_j = a_{jk}$ and $C=c_i$
        > $n:$ number of training examples for which $C=c_i$
        > $p:$ prior estimate (usually, $p = \frac{1}{t}$ for $t$ possible values os $X_j$)
        > $m:$ weight to prior(number of "virtual" examples, $m\ge 1$)

## Summary
- Naïve Bayes: the conditional independence assumption
    - Training is very easy and fast; just requiring considering each  attribute in each class separately
    - Test is straightforward; just looking up tables or calculating conditional probabilities with estimated distributions 
- A popular generative model
    - Performance competitive to most of state-of-the-art classifiers even in presence of violating independence assumption
    - Many successful applications, e.g., spam mail filtering
    - A good candidate of a base learner in ensemble learning
    - Apart from classification, naïve Bayes can do more… 


## Explanation from [Thuc](https://people.unisa.edu.au/thuc.le)
<YouTube id="w_bPyypZiyo" />

## References
01. **Naive Bayes Classifier**
    ::: details Resource from Youtube
    <YouTube id="AqonCeZUcC4" />
    :::
02. **Week 2 Slides from Thuc (SP52023)**