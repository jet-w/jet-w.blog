---
title: 02.Bayesian networks(W3)
index: true
icon: "/assets/icon/common/data-mining.svg"
icon-size: "4rem"
author: Haiyue
date: 2023-08-17
category:
  - Guide
---

## Bayesian networks: examples, definition
::: details
::: tabs
@tab Bayesian network 1
![Bayesian Network 1](/data/unisa/AdvancedAnalytic2/week3/BayesianNetwork1.png)
@tab Bayesian network 2
![Bayesian Network 2](/data/unisa/AdvancedAnalytic2/week3/BayesianNetwork2.png)
:::

## Bayesian networks
### What is a BN?
**A Bayesian network (BN) is a <span style="color:red;font-weight:bold"> graphical model</span> for depicting probabilistic relationships among a set of variables.**
* BN Encodes the <span style="color:orange;font-weight:bold">conditional independence relationships</span> between the variables in the graph structure. 
* Provides a <span style="color:orange;font-weight:bold">compact representation of the joint probability distribution</span> over the variables 
* A problem domain is modeled by a list of variables $X_1, …, X_n$ 
* Knowledge about the problem domain is represented by a joint probability $P(X_1, …, X_n)$
* <span style="color:orange;font-weight:bold">Directed links represent causal direct influences</span>
* Each node has a <span style="color:orange;font-weight:bold">conditional probability table</span> quantifying the effects from the parents. 
* <span style="color:red;font-weight:bold">No directed cycles</span>
::: info Two important properties of Bayesian Networks
01. <span style="color:red;font-weight:bold">Encodes the conditional independence relationships</span> between the variables in the graph structure
02. Is a <span style="color:red;font-weight:bold">compact representation</span> of the joint probability distribution over the variables

:::

### Components of A Bayesian Network
A Bayesian Network is consisted with two components
1. **A directecd Acyclic Graph**
    ```mermaid
    ---
    title: A directecd Acyclic Graph
    ---
    flowchart TB
        A-->B-->C & D
    ```
    ::: info Characteristics
    01. Each node in the graph is a random variable
    02. A node X is a parent of another node Y if there is an arrow from node X to node Y 
        **eg.** A is a parent of B 
    03. Informally, an arrow from node X to node Y means X has a <span style="color:orange;font-weight:bold;">direct influence</span> on Y
    :::
2. A set of **tables for each node** in the graph
![Alt text](/data/unisa/AdvancedAnalytic2/week3/TablesForBayesianNetwork.png)
    ::: info Characteristics
    01. Each node $X_i$ has a <span style="color:orange;font-weight:bold;">conditional probability distribution</span> $P(X_i | Parents(X_i))$ that quantifies the effect of the parents on the node
    02. The parameters are the probabilities in these conditional probability tables (CPTs)
    :::
#### Bayesian Network Example
Using the network in the example, suppose you want to calculate:
$$
\begin{equation}
\begin{split}   
P(A = true, B = true, C &= true, D = true) \\
                        &= P(A = true) * P(B = true | A = true) \\
                        &\text{\ \ \ \ } * P(C = true | B = true) P( D = true | B = true) \\
                        & = (0.4)*(0.3)*(0.1)*(0.95)
\end{split}
\end{equation}     
$$
Details for data refer to [here.](#components-of-a-bayesian-network)

### Conditional Independence
[The Markov condition](https://en.wikipedia.org/wiki/Causal_Markov_condition): Given its <span style="color:orange;font-weight:bold;">parents (P1, P2)</span>, a node <span style="color:red;font-weight:bold;">(X)</span> is <span style="color:orange;font-weight:bold;">conditionally independent</span> of its <span style="color:green;font-weight:bold;">non-descendants</span> (ND1, ND2). 
![The Markov condition](/data/unisa/AdvancedAnalytic2/week3/Markov.png =400x)

![](/data/unisa/AdvancedAnalytic2/week3/BayesianNetwork1.png =400x){hidden: "hidden"}


#### The Joint Probability Distribution

Due to the <span style="color:orange">Markov condition</span>, we can compute the joint probability distribution over all the variables <span style="color:orange">$X_1, …, X_n$</span> in the Bayesian net using the formula:

::: center
$P(X_1 = x_1, ..., X_n=x_n) = \prod_{i=1}^{n}P(X_i=x_i|Parents(X_i))$
:::
::: info
Where $Parents(X_i)$ means the values of the Parents of the node $X_i$ with respect to the graph 
:::


## Bayesian network inference
* <span style="color:orange;font-weight:bold;">Using a Bayesian network</span> to <span style="color:orange;font-weight:bold;">compute probabilities</span> is called inference
* In general, inference involves queries of the form:	<span style="color:orange;font-weight:bold;">$P( X | E )$</span>
    > ***E***: The evidence variable(s)
    > ***X***: The query variable(s)

* Diagnostic (evidential, abductive): <span style="color:orange">from effect to cause</span>
    - P(Buglary|JonhCalls), P(B|J)=0.016
    - P(B|J,M)=0.29
    - P(A|J,M)=0.76
* Causal (predictive): From cause to effect
    - P(J|B)=0.86
    - P(M|B)=0.67
* Intercausal (explaining away): common effect
    - P(B|A)=0.38
    - P(B|A, E)=0.003
* Mixed: two or more of the above combined
    - P(A|J,E’)=0.03
    - P(B|J,E’)=0.017

* In general, the problem of Bayesian network inference is NP-hard (exponential in the size of the graph)

* Exact inference 
    - Probability and Markov condition
    - Variable elimination
    - Clustering / join tree algorithms

* Approximate inference
    - Stochastic simulation / sampling methods
    - Markov chain Monte Carlo methods
    - Genetic algorithms
    - Neural networks
    - Simulated annealing
    - Mean field theory


* $P(X|E) = \frac{P(E|X)P(X)}{P(E)} = \frac{P(E,X)}{P(E)}$

* Marginalisation: $P(x) = \sum_yP(x,y)$
    e.g. $P(X,E) = P(X,E,A) + P(X,E,A')$, A is binary

* Markov condition: $I(X, NDx|Parent(X))$

* Diagnostic (evidential, abductive): from effect to cause
![](/data/unisa/AdvancedAnalytic2/week3/BayesianNetwork1.png =400x)

$$
\begin{equation}
\begin{split}
P(B|J) &= \frac{P(B,J)}{P(J)}\\
\\
P(B,J) &= P(B,J,A) + P(B,J,A')\\
       &= P(J|A,B)P(A,B) + P(J|A',B)P(A'B)\\
       &= P(J|A)P(A,B) + P(J|A')P(A',B)\\
       &= 0.9P(A,B) + 0.05P(A',B)
\end{split}
\end{equation}
$$

$$
\begin{equation}
\begin{split}
P(A,B) &= P(A,B,E) + P(A,B,E')\\
       &= P(A|B,E)P(B,E) + P(A|B,E')P(B,E')\\
       &= P(A|B,E)P(B)P(E) + P(A|B,E')P(B)P(E')\\
       &= 0.95*0.001*0.002 + 0.95*0.001*0.998
\end{split}
\end{equation}
$$

---
$$
\begin{equation}
\begin{split}
P(B|J=T) &= \frac{P(B,J)}{P(J)}\\
 \\
P(J) &= P(J,A) + P(J,A')\\
     &= P(J|A)P(A') + P(J|A')P(A')\\
     &= P(A|B,E)P(B)P(E) + P(A|B,E')P(B)P(E')\\
     &= 0.9P(A) + 0.05P(A')\\
     \\
P(A) &= P(A,B,E) + P(A,B,E') + P(A, B',E) + P(A,B',E') \\
     &= P(A| B,E)P(B,E) + ...\\
     &=0.0025
\end{split}
\end{equation}
$$

$P(J) = 0.05212$
$P(B|J) = \frac{P(B,J)}{P(J)}$ = 0.016

$P(J|B) = \frac{P(B,J)}{P(B)}$
$P(B,J) = P(B,J,A) + P(B,J,A') = 0.00086$
$P(B) = 0.001$
$P(J|B) = \frac{0.00086}{0.001} = 0.86$

* Intercausal (explaining away): common effect
$P(B|A) = \frac{P(A,B)}{P(A)}$
$P(B,A) = 0.00095$
$P(A) = 0.0025$
$P(B|A) = \frac{0.00095}{0.0025} = 0.38$

## Summary
* Bayesian networks
    * Definition
    * Properties: <span style="color:orange">Markov condition</span>, <span style="color:orange">compact joint probability</span>

* Bayesian networks inference
    * Understand how to use <span style="color:orange">probability theory</span> and <span style="color:orange">Markov condition</span> to do the inference for different types of queries
    * In practice, we are going to use R (gRain package)

* Next week, learning Bayesian networks from data







## References
01. **Naive Bayes Classifier**
    ::: details Resource from Youtube
    <YouTube id="0MCMsdPKLyQ" />
    :::
02. **Week 3 Slides from [Thuc](https://people.unisa.edu.au/thuc.le) (SP52023)**