---
title: 01.Linear Regression
index: true
icon: "/assets/icon/common/slides.svg"
author: Haiyue
date: 2023-08-02
category:
  - math
tag:
  - linear regression
---

## [Definition of Regression](https://www.investopedia.com/terms/r/regression.asp#:~:text=A%20regression%20is%20a%20statistical,more%20of%20the%20explanatory%20variables.)
Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).

## Understanding Regression
Regression **captures the correlation** between variables observed in a data set and quantifies whether those correlations are statistically significant or not.

The two basic types of regression are ***simple* linear regression** and ***multiple* linear regression**, although there are non-linear regression methods for more complicated data and analysis. Simple linear regression uses one independent variable to explain or [predict the outcome of the dependent](https://www.investopedia.com/terms/s/stepwise-regression.asp) variable Y, while multiple linear regression uses two or more independent variables to predict the outcome (while holding all others constant).

Regression can help finance and investment professionals as well as professionals in other businesses. Regression can also help predict sales for a company based on weather, previous sales, GDP growth, or other types of conditions. [The capital asset pricing model (CAPM)](https://www.investopedia.com/terms/c/capm.asp) is an often-used regression model in finance for pricing assets and discovering the costs of capital.


## Linear Regression
Linear regression establishes the linear relationship between two variables based on [a line of best fit](#line-of-best-fit).

## [Line of Best Fit](https://www.investopedia.com/terms/l/line-of-best-fit.asp)
Line of best fit refers to a line through a scatter plot of data points that best expresses the relationship between those points. Statisticians typically use the least squares method (sometimes known as ordinary least squares, or OLS) to arrive at the geometric equation for the line, either through manual calculations or by using software.
::: info <span style="font-weight: bold;">Key Takeaway</span>
- A line of best fit is a straight line that minimizes the distance between it and some data.
- The line of best fit is used to express a relationship in a scatter plot of different data points.
- It is an output of regression analysis and can be used as a prediction tool for indicators and price movements.
- In finance, the line of best fit is used to identify trends or correlations in market returns between assets or over time.
:::

## The hypothesis for the linear regression
01. **There must be a <span style="color:red">linear relationship</span> between the <span style="color:red">independent variable</span> and the <span style="color:red">dependent variable</span>**
    - Evaluating using scatter plot/lowess chart
    - Could using other methods to do the relationship if not fullfil the hypothesis
        - **Spline**: piecewise
        - **Quadratic**: $a\cdot x^2 + b\cdot x$
        - **Cubic**: $a\cdot x^3 + b\cdot x^2 + c\cdot x$
        - **Restricted cubic**: constrained to be linear in the two tails
        - **Restricted cubic spline**: a way of testing the hypothesis that the relationship is not linear or summarizing a relationship that is too non-linear to be usefully summarized by a linear relationship. 
        - etc.
02. **Independent observations**
    - Evaluating using Durbin-Watson
    - Normally, it should be satisfied during design stage.
    - For dependent observations could using *GEE* model or *Multi-level* model.
03. **There are no significant outliers**
    - Evaluating using Boxplot or violin plot
04. **Equal variance**
    - Evaluating using residual-versus-fitted plot
05. **Regression residuals are <span style="color:red">approximately</span> normally distributed**
    - Evaluating using histogram/qqplot/skewness/kurtosis/shapiro-wilk/shapiro-francia






## The basic definition

**Residual**: The residual for each observation is the <span style="color:red">difference</span> between  <span style="color:red">predicted values of y</span> (dependent variable) and  <span style="color:red">observed values of y</span>.

---
**[Intercept](https://byjus.com/maths/intercept/)**:
In Maths, an intercept is <span style="color:red">a point on the y-axis</span>, through which the slope of the line passes. It is the y-coordinate of a point where a straight line or a curve intersects the y-axis. This is represented when we write the equation for a line, y = mx+c, where m is slope and c is the y-intercept.

---
::: info More INFO
**There are basically two intercepts**, x-intercept and y-intercept. The point where the line crosses the x-axis is the x-intercept and the point where the line crosses the y-axis is the y-intercept. In this article, you will learn what is the intercept, how to find the intercept for a given line, graphing intercepts along with solved examples.
:::

---
**[Sum of squares(SS)](https://corporatefinanceinstitute.com/resources/data-science/sum-of-squares/)**:
Sum of squares (SS) is a statistical tool that is used to identify the dispersion of data as well as how well the data can fit the model in regression analysis. The sum of squares got its name because it is calculated by finding the sum of the squared differences.

---
**[Mean Squared Errors(MS)](https://www.freecodecamp.org/news/https-medium-com-sharadvm-how-to-read-a-regression-table-661d391e9bd7-708e75efc560/#:~:text=Mean%20Squared%20Errors%20(MS)%20%E2%80%94,for%20both%2C%20regression%20and%20residuals.&text=This%20is%20otherwise%20calculated%20by,residual%20df%20in%20denominator%20degrees.)**
Mean Squared Errors (MS) — are the mean of the sum of squares or the sum of squares divided by the <span style="color:red">degrees of freedom</span> for both, regression and residuals.

Regression MS $=\frac{\sum(\hat{y}-\bar{y})^2}{Reg.df}$
Residual MS $=\frac{\sum(y-\hat{y})^2}{Reg.df}$

---
::: 
$\hat{y}$: [an estimator or an estimated value](https://en.wikipedia.org/wiki/Hat_operator#:~:text=6%20See%20also-,Estimated%20value,(the%20statistical%20errors).).
$\bar{y}$: [sample mean of a random variable.](https://abstractmath.org/MM/MMOtherSymbols.htm#:~:text=in%20arrow%20notation.-,Bar,substructure%20of%20a%20mathematical%20structure.)
[Degrees of freedom](https://statisticsbyjim.com/hypothesis-testing/degrees-freedom-statistics/): Degrees of freedom are the number of independent values that a statistical analysis can estimate. 

---
**[F](https://www.freecodecamp.org/news/https-medium-com-sharadvm-how-to-read-a-regression-table-661d391e9bd7-708e75efc560/#:~:text=Mean%20Squared%20Errors%20(MS)%20%E2%80%94,for%20both%2C%20regression%20and%20residuals.&text=This%20is%20otherwise%20calculated%20by,residual%20df%20in%20denominator%20degrees.)**: F — is used to test the hypothesis that the slope of the independent variable is zero. Mathematically, it can also be calculated as
$F = \frac{\text{Regression MS}}{\text{Residual MS}}$


## Calculation
Linear regression models often use a least-squares approach to determine the line of best fit. The least-squares technique is determined by minimizing the sum of squares created by a mathematical function. A square is, in turn, determined by squaring the distance between a data point and the regression line or mean value of the data set.

Once this process has been completed (usually done today with software), a regression model is constructed. The general form of each type of regression model is:

### Simple linear regression:
$Y=a+bX+\mu$

### Multiple linear regression:
$Y=a+b_1X_1+b_2X_2 + b_3X_3+...+b_tX_t+\mu$
**where**
  - $Y= \text{The dependent variable you are trying to predict or explain}$
  - $X= \text{The explanatory (independent) variable(s) you are using to predict or associate with }Y$
  - $a = \text{The y-intercept}$
  - $b= \text{(beta coefficient) is the slope of the explanatory variable(s)}$
  - $u= \text{The regression residual or error term}$
​


## References
01. [Linear Regression](https://www.youtube.com/watch?v=LeY0u56Xuqs)
02. [Linear Regression using Excel](https://www.investopedia.com/ask/answers/062215/how-can-i-run-linear-and-multiple-regressions-excel.asp)
03. [Linear Regression using Python](https://realpython.com/linear-regression-in-python/)
04. [What is Regression? Definition, Calculation, and Example](https://www.investopedia.com/terms/r/regression.asp#:~:text=A%20regression%20is%20a%20statistical,more%20of%20the%20explanatory%20variables.)
05. [Line of Best Fit: Definition, How It Works, and Calculation](https://www.investopedia.com/terms/l/line-of-best-fit.asp)






